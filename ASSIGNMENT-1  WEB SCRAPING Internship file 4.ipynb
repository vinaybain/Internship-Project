{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8d48a87",
   "metadata": {},
   "source": [
    "# 1)\tWrite a python program to display IMDB’s Top rated 100 Indian movies’ data https://www.imdb.com/list/ls056092300/ (i.e. name, rating, year ofrelease) and make data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f16b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Name Rating Year of Release\n",
      "0                     Ship of Theseus      8            2012\n",
      "1                              Iruvar    8.4            1997\n",
      "2                     Kaagaz Ke Phool    7.8            1959\n",
      "3   Lagaan: Once Upon a Time in India    8.1            2001\n",
      "4                     Pather Panchali    8.2            1955\n",
      "..                                ...    ...             ...\n",
      "95                        Apur Sansar    8.4            1959\n",
      "96                        Kanchivaram    8.2            2008\n",
      "97                    Monsoon Wedding    7.3            2001\n",
      "98                              Black    8.1            2005\n",
      "99                            Deewaar      8            1975\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_imdb_top_100_indian_movies():\n",
    "    # URL of the IMDb list with movie details\n",
    "    url = \"https://www.imdb.com/list/ls056092300/\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all movie entries\n",
    "        movie_entries = soup.find_all('div', class_='lister-item-content')\n",
    "\n",
    "        # Initialize lists to store movie details\n",
    "        names = []\n",
    "        ratings = []\n",
    "        years = []\n",
    "\n",
    "        # Extract data for each movie\n",
    "        for movie in movie_entries:\n",
    "            # Movie name\n",
    "            name = movie.h3.a.text.strip()\n",
    "            names.append(name)\n",
    "\n",
    "            # Movie rating\n",
    "            rating = movie.find('span', class_='ipl-rating-star__rating').text.strip()\n",
    "            ratings.append(rating)\n",
    "\n",
    "            # Year of release\n",
    "            year = movie.find('span', class_='lister-item-year').text.strip('()')\n",
    "            years.append(year)\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame({'Name': names, 'Rating': ratings, 'Year of Release': years})\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to fetch the webpage. Status code:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "# Call the function to scrape IMDb top 100 Indian movies' details\n",
    "imdb_top_100_indian_movies_df = scrape_imdb_top_100_indian_movies()\n",
    "\n",
    "# Print the DataFrame\n",
    "print(imdb_top_100_indian_movies_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43385d77",
   "metadata": {},
   "source": [
    "# 4)\tWrite a python program to scrape details of all the posts from https://www.patreon.com/coreyms .Scrape the heading, date, content and the likes for the video from the link for the youtube video from the post. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2f6a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_patreon_posts():\n",
    "    # URL of the webpage with posts\n",
    "    url = \"https://www.patreon.com/coreyms\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all post entries\n",
    "        post_entries = soup.find_all('div', class_='post-container')\n",
    "\n",
    "        # Initialize lists to store post details\n",
    "        headings = []\n",
    "        dates = []\n",
    "        contents = []\n",
    "        likes = []\n",
    "\n",
    "        # Extract data for each post\n",
    "        for post in post_entries:\n",
    "            # Heading\n",
    "            heading = post.find('h2', class_='post-header__title').text.strip()\n",
    "            headings.append(heading)\n",
    "\n",
    "            # Date\n",
    "            date = post.find('time', class_='date').text.strip()\n",
    "            dates.append(date)\n",
    "\n",
    "            # Content\n",
    "            content = post.find('div', class_='post-content').text.strip()\n",
    "            contents.append(content)\n",
    "\n",
    "            # Likes (if available)\n",
    "            like = post.find('span', class_='icon-like-filled')\n",
    "            if like:\n",
    "                likes.append(like.next_sibling.strip())\n",
    "            else:\n",
    "                likes.append('Not available')\n",
    "\n",
    "        # Return the lists of post details\n",
    "        return headings, dates, contents, likes\n",
    "    else:\n",
    "        print(\"Failed to fetch the webpage. Status code:\", response.status_code)\n",
    "        return None, None, None, None\n",
    "\n",
    "# Call the function to scrape post details\n",
    "headings, dates, contents, likes = scrape_patreon_posts()\n",
    "\n",
    "# Print the details\n",
    "for i in range(len(headings)):\n",
    "    print(\"Heading:\", headings[i])\n",
    "    print(\"Date:\", dates[i])\n",
    "    print(\"Content:\", contents[i])\n",
    "    print(\"Likes:\", likes[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aabc43",
   "metadata": {},
   "source": [
    "# 5)\tWrite a python program to scrape house details from mentioned URL. It should include house title, location, area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar, Rajaji Nagar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db0a68df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping house details for Indira-nagar:\n",
      "Scraping house details for Jayanagar:\n",
      "Scraping house details for Rajaji-nagar:\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_house_details(locality):\n",
    "    # URL of the webpage with house details\n",
    "    url = f\"https://www.nobroker.in/property/sale/{locality}/?searchParam=W3sibGF0IjoxOS4zNDU0NzY3LCJsb24iOjc3LjY0NjAwOTUsInBsYWNlSWQiOiJDaElKMmJFcHhzQkJPSlNSV3loemM0cENlOWwiLCJwbGFjZU5hbWUiOiJDaElKMmJFcHhzQkJPSlNSV3loemM0cENlOWwifV0=&radius=2.0\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all house entries\n",
    "        house_entries = soup.find_all('div', class_='card')\n",
    "\n",
    "        # Initialize lists to store house details\n",
    "        titles = []\n",
    "        locations = []\n",
    "        areas = []\n",
    "        emis = []\n",
    "        prices = []\n",
    "\n",
    "        # Extract data for each house\n",
    "        for house in house_entries:\n",
    "            # Title\n",
    "            title = house.find('h2', class_='heading-6 font-semi-bold nb__1AShY').text.strip()\n",
    "            titles.append(title)\n",
    "\n",
    "            # Location\n",
    "            location = house.find('div', class_='nb__2CMjv').text.strip()\n",
    "            locations.append(location)\n",
    "\n",
    "            # Area\n",
    "            area = house.find('div', class_='nb__3oNyC').text.strip()\n",
    "            areas.append(area)\n",
    "\n",
    "            # EMI\n",
    "            emi = house.find('div', class_='font-semi-bold heading-6', text='₹').next_sibling.strip()\n",
    "            emis.append(emi)\n",
    "\n",
    "            # Price\n",
    "            price = house.find('div', class_='font-semi-bold heading-6').text.strip()\n",
    "            prices.append(price)\n",
    "\n",
    "        # Return the lists of house details\n",
    "        return titles, locations, areas, emis, prices\n",
    "    else:\n",
    "        print(f\"Failed to fetch the webpage for {locality}. Status code:\", response.status_code)\n",
    "        return None, None, None, None, None\n",
    "\n",
    "# List of localities\n",
    "localities = ['indira-nagar', 'jayanagar', 'rajaji-nagar']\n",
    "\n",
    "# Scrape house details for each locality\n",
    "for locality in localities:\n",
    "    print(f\"Scraping house details for {locality.capitalize()}:\")\n",
    "    titles, locations, areas, emis, prices = scrape_house_details(locality)\n",
    "    \n",
    "    # Print the details\n",
    "    for i in range(len(titles)):\n",
    "        print(\"Title:\", titles[i])\n",
    "        print(\"Location:\", locations[i])\n",
    "        print(\"Area:\", areas[i])\n",
    "        print(\"EMI:\", emis[i])\n",
    "        print(\"Price:\", prices[i])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f023adb6",
   "metadata": {},
   "source": [
    "# 6)\tWrite a python program to scrape first 10 product details which include product name , price , Image URL from https://www.bewakoof.com/bestseller?sort=popular . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdff0397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_product_details():\n",
    "    # URL of the webpage with product details\n",
    "    url = \"https://www.bewakoof.com/bestseller?sort=popular\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all product entries\n",
    "        product_entries = soup.find_all('div', class_='productCardWrapper-2RwRzX')\n",
    "\n",
    "        # Initialize lists to store product details\n",
    "        product_names = []\n",
    "        prices = []\n",
    "        image_urls = []\n",
    "\n",
    "        # Extract data for each product (up to first 10)\n",
    "        for product in product_entries[:10]:\n",
    "            # Product name\n",
    "            name = product.find('p', class_='productCardName').text.strip()\n",
    "            product_names.append(name)\n",
    "\n",
    "            # Price\n",
    "            price = product.find('p', class_='productCardPrice').text.strip()\n",
    "            prices.append(price)\n",
    "\n",
    "            # Image URL\n",
    "            image_div = product.find('div', class_='productCardImageWrapper-2N53yh')\n",
    "            image_url = image_div.find('img')['src']\n",
    "            image_urls.append(image_url)\n",
    "\n",
    "        # Return the lists of product details\n",
    "        return product_names, prices, image_urls\n",
    "    else:\n",
    "        print(\"Failed to fetch the webpage. Status code:\", response.status_code)\n",
    "        return None, None, None\n",
    "\n",
    "# Call the function to scrape product details\n",
    "product_names, prices, image_urls = scrape_product_details()\n",
    "\n",
    "# Print the details\n",
    "for i in range(len(product_names)):\n",
    "    print(\"Product Name:\", product_names[i])\n",
    "    print(\"Price:\", prices[i])\n",
    "    print(\"Image URL:\", image_urls[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6570d29",
   "metadata": {},
   "source": [
    "# 7)\tPlease visit https://www.cnbc.com/world/?region=world and scrap- a) headings b)\tdate c)\tNews link \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "245de85d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Call the function to scrape news details\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m headings, dates, news_links \u001b[38;5;241m=\u001b[39m scrape_cnbc_news()\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Print the details\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(headings)):\n",
      "Cell \u001b[1;32mIn[19], line 27\u001b[0m, in \u001b[0;36mscrape_cnbc_news\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Extract data for each news entry\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m news_entries:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Heading\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     heading\u001b[38;5;241m=\u001b[39m entry\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCard-titleLink\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     28\u001b[0m     headings\u001b[38;5;241m.\u001b[39mappend(heading)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Date\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_cnbc_news():\n",
    "    # URL of the webpage with news details\n",
    "    url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all news entries\n",
    "        news_entries = soup.find_all('div', class_='Card-titleContainer')\n",
    "\n",
    "        # Initialize lists to store news details\n",
    "        headings = []\n",
    "        dates = []\n",
    "        news_links = []\n",
    "\n",
    "        # Extract data for each news entry\n",
    "        for entry in news_entries:\n",
    "            # Heading\n",
    "            heading= entry.find('a', class_='Card-titleLink').text.strip()\n",
    "            headings.append(heading)\n",
    "\n",
    "            # Date\n",
    "            date = entry.find('span', class_='Card-time').text.strip()\n",
    "            dates.append(date)\n",
    "\n",
    "            # News link\n",
    "            news_link = \"https://www.cnbc.com\" + entry.find('a', class_='Card-titleLink')['href']\n",
    "            news_links.append(news_link)\n",
    "\n",
    "        # Return the lists of news details\n",
    "        return headings, dates, news_links\n",
    "    else:\n",
    "        print(\"Failed to fetch the webpage. Status code:\", response.status_code)\n",
    "        return None, None, None\n",
    "\n",
    "# Call the function to scrape news details\n",
    "headings, dates, news_links = scrape_cnbc_news()\n",
    "\n",
    "# Print the details\n",
    "for i in range(len(headings)):\n",
    "    print(\"Heading:\", headings[i])\n",
    "    print(\"Date:\", dates[i])\n",
    "    print(\"News Link:\", news_links[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b52ad4",
   "metadata": {},
   "source": [
    "# 8)\tPlease visit https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-     articles/ and scrap-          a) Paper title b)\tdate c)\tAuthor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bc925a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_most_downloaded_articles():\n",
    "    # URL of the webpage with article details\n",
    "    url = \"https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all article entries\n",
    "        article_entries = soup.find_all('div', class_='list-item-content')\n",
    "\n",
    "        # Initialize lists to store article details\n",
    "        paper_titles = []\n",
    "        dates = []\n",
    "        authors = []\n",
    "\n",
    "        # Extract data for each article entry\n",
    "        for entry in article_entries:\n",
    "            # Paper title\n",
    "            paper_title = entry.find('a', class_='title').text.strip()\n",
    "            paper_titles.append(paper_title)\n",
    "\n",
    "            # Date\n",
    "            date = entry.find('span', class_='date').text.strip()\n",
    "            dates.append(date)\n",
    "\n",
    "            # Author\n",
    "            author = entry.find('span', class_='author').text.strip()\n",
    "            authors.append(author)\n",
    "\n",
    "        # Return the lists of article details\n",
    "        return paper_titles, dates, authors\n",
    "    else:\n",
    "        print(\"Failed to fetch the webpage. Status code:\", response.status_code)\n",
    "        return None, None, None\n",
    "\n",
    "# Call the function to scrape article details\n",
    "paper_titles, dates, authors = scrape_most_downloaded_articles()\n",
    "\n",
    "# Print the details\n",
    "for i in range(len(paper_titles)):\n",
    "    print(\"Paper Title:\", paper_titles[i])\n",
    "    print(\"Date:\", dates[i])\n",
    "    print(\"Author:\", authors[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b068a498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
