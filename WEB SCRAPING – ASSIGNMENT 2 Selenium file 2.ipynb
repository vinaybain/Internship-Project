{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d80ac248",
   "metadata": {},
   "source": [
    "Q1: In this question you have to scrape data using the filters available on the webpage You have to use the location and salary filter.  \n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.  \n",
    "You have to scrape the job-title, job-location, company name, experience required.  The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs  \n",
    "The task will be done as shown in the below steps:  \n",
    "1.\tfirst get the web page https://www.naukri.com/ \n",
    "2.\tEnter “Data Scientist” in “Skill, Designations, and Companies” field.  \n",
    "3.\tThen click the search button.  \n",
    "4.\tThen apply the location filter and salary filter by checking the respective boxes  \n",
    "5.\tThen scrape the data for the first 10 jobs results you get.  \n",
    "6.\tFinally create a dataframe of the scraped data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba66407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\h.p\\anaconda3\\lib\\site-packages (4.20.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1864c4d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'job_company' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m company_tags:\n\u001b[0;32m     39\u001b[0m     company\u001b[38;5;241m=\u001b[39mi\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m---> 40\u001b[0m     job_company\u001b[38;5;241m.\u001b[39mappend(company)\n\u001b[0;32m     42\u001b[0m experience_tags\u001b[38;5;241m=\u001b[39mdriver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mXPATH,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//span[@class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mni-job-tuple-icon ni-job-tuple-icon-srp-experience exp\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m experience_tags:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'job_company' is not defined"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Open the webpage\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "# Step 2: Enter search criteria\n",
    "job_title=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "job_title.send_keys(\"Data Scientist\")\n",
    "\n",
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[7]/div/div/div[5]/div/div/div/div[1]/div/input\")\n",
    "location.send_keys(\"Delhi/NCR\")\n",
    "\n",
    "# Step 3: Click the search button\n",
    "search_button=driver.find_element(By.CLASS_NAME,'qsbSubmit')\n",
    "search_button.click()\n",
    "\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]\n",
    "salary_filter=[]\n",
    "\n",
    "title_tags=driver.find_elements(By.XPATH,'//div[@class=\"cust-job-tuple layout-wrapper lay-2 sjw__tuple \"]/div/a')\n",
    "for i in title_tags:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "location_tags=driver.find_elements(By.XPATH,'//span[@class=\"ni-job-tuple-icon ni-job-tuple-icon-srp-location loc\"]')\n",
    "for i in location_tags:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "company_tags=driver.find_elements(By.XPATH,'//div[@class=\" row2\"]/span/a[1]')\n",
    "for i in company_tags:\n",
    "    company=i.text\n",
    "    job_company.append(company)\n",
    "    \n",
    "experience_tags=driver.find_elements(By.XPATH,'//span[@class=\"ni-job-tuple-icon ni-job-tuple-icon-srp-experience exp\"]')\n",
    "for i in experience_tags:\n",
    "    exp=i.text\n",
    "    experience_required.append(exp)\n",
    "    \n",
    "salary_filter= driver.find_element(By.XPATH,'//span[@class=\"sal-wrap ver-line\"]')\n",
    "for i in salary_tags:\n",
    "    salary=i.text\n",
    "    job_salary.append(salary)\n",
    "    \n",
    "    print(len(job_title),len(job_location),len(company_name),len(experience_required),len(salary_filter))\n",
    "    \n",
    "    df=pd.DataFrame({'title':job_title,'Location':job_location,'Company_name':company_name,'Experience':experience_required,'Salary':salary_filter})\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c542d0a",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data. \n",
    " This task will be done in following steps:\n",
    " \n",
    "1.\tFirst get the webpage https://www.shine.com/ \n",
    "2.\tEnter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field. \n",
    "3.\tThen click the searchbutton.  \n",
    "4.\tThen scrape the data for the first 10 jobs results you get.  \n",
    "5.\tFinally create a dataframe of the scraped data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4a51d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Open the webpage\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.shine.com/\")\n",
    "\n",
    "job_title=driver.find_element(By.CLASS_NAME,\"id_q\")\n",
    "job_title.send_keys(\"Data Scientist\")\n",
    "\n",
    "location=driver.find_element(By.XPATH,\"/html/body/div/div[4]/div/div[2]/div[2]/div/form/div/div[1]/ul/li[2]/div/input\")\n",
    "location.send_keys(\"Bangalore\")\n",
    "\n",
    "# Step 3: Click the search button\n",
    "search_button=driver.find_element(By.CLASS_NAME,\"btn btn-secondary undefined\")\n",
    "search_button.click()\n",
    "\n",
    "\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"job_title_anchor\"]')\n",
    "for i in title_tags:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "location_tags=driver.find_elements(By.XPATH,'//li[@class=\"w-30 mr-10 result-display-location\"]/span')\n",
    "for i in location_tags:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"result-display-company-name\"]')\n",
    "for i in company_tags:\n",
    "    company=i.text\n",
    "    job_company.append(company)\n",
    "    \n",
    "experience_tags=driver.find_elements(By.XPATH,'//li[@class=\"w-30 mr-10 result-display-exp\"]/span')\n",
    "for i in experience_tags:\n",
    "    exp=i.text\n",
    "    experience_required.append(exp) \n",
    "    \n",
    "    print(len(job_title),len(job_location),len(company_name),len(experience_required))\n",
    "    \n",
    "    df=pd.DataFrame({'title':job_title,'Location':job_location,'Company_name':company_name,'Experience':experience_required})\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b5e29",
   "metadata": {},
   "source": [
    "Q3: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb/product-\n",
    "reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=F LIPKART \n",
    "  \n",
    "  \n",
    " \n",
    "As shown in the above page you have to scrape the tick marked attributes. These are: \n",
    "1.\tRating \n",
    "2.\tReview summary \n",
    "3.\tFull review \n",
    "4.\tYou have to scrape this data for first 100reviews. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79b93249",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m url\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Scrape reviews\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m all_reviews \u001b[38;5;241m=\u001b[39m scrape_reviews(url)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Create dataframe\u001b[39;00m\n\u001b[0;32m     45\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_reviews)\n",
      "Cell \u001b[1;32mIn[41], line 30\u001b[0m, in \u001b[0;36mscrape_reviews\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     28\u001b[0m all_reviews \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(all_reviews) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m100\u001b[39m:\n\u001b[1;32m---> 30\u001b[0m     all_reviews\u001b[38;5;241m.\u001b[39mextend(scrape_reviews_from_page(driver))\n\u001b[0;32m     31\u001b[0m     driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.scrollTo(0, document.body.scrollHeight);\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m     WebDriverWait(driver, \u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39minvisibility_of_element_located((By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloading\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "Cell \u001b[1;32mIn[41], line 17\u001b[0m, in \u001b[0;36mscrape_reviews_from_page\u001b[1;34m(driver)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m review_elements:\n\u001b[0;32m     16\u001b[0m     rating \u001b[38;5;241m=\u001b[39m review\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 17\u001b[0m     review_summary \u001b[38;5;241m=\u001b[39m review\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_2-N8zT\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     18\u001b[0m     full_review \u001b[38;5;241m=\u001b[39m review\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt-ZTKy\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdiv\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     19\u001b[0m     reviews\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRating\u001b[39m\u001b[38;5;124m'\u001b[39m: rating, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview Summary\u001b[39m\u001b[38;5;124m'\u001b[39m: review_summary, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFull Review\u001b[39m\u001b[38;5;124m'\u001b[39m: full_review})\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape reviews from a page\n",
    "def scrape_reviews_from_page(driver):\n",
    "    reviews = []\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'row')))\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    review_elements = soup.find_all('div', class_='row')\n",
    "\n",
    "    for review in review_elements:\n",
    "        rating = review.find('div', class_='row').find('div').text.strip()\n",
    "        review_summary = review.find('p', class_='_2-N8zT').text.strip()\n",
    "        full_review = review.find('div', class_='t-ZTKy').div.text.strip()\n",
    "        reviews.append({'Rating': rating, 'Review Summary': review_summary, 'Full Review': full_review})\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Main function to scrape reviews\n",
    "def scrape_reviews(url):\n",
    "    driver = webdriver.Chrome()  # Change this to the path of your WebDriver executable\n",
    "    driver.get(url)\n",
    "\n",
    "    all_reviews = []\n",
    "    while len(all_reviews) < 100:\n",
    "        all_reviews.extend(scrape_reviews_from_page(driver))\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        WebDriverWait(driver, 10).until(EC.invisibility_of_element_located((By.CLASS_NAME, 'loading')))\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return all_reviews[:100]\n",
    "\n",
    "# URL of the Flipkart page\n",
    "url= \"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\"\n",
    "\n",
    "# Scrape reviews\n",
    "all_reviews = scrape_reviews(url)\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(all_reviews)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('iphone11_reviews_selenium.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16bfeb12",
   "metadata": {},
   "source": [
    "Q4: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field. \n",
    "You have to scrape 3 attributes of each sneaker: \n",
    "1.\tBrand \n",
    "2.\tProduct Description \n",
    "3.\tPrice \n",
    "As shown in the below image, you have to scrape the above attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fd22fe6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_element_by_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msneakers_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 43\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[47], line 38\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m---> 38\u001b[0m     sneakers\u001b[38;5;241m=\u001b[39m scrape_sneakers()\n\u001b[0;32m     39\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(sneakers)\n\u001b[0;32m     40\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msneakers_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[47], line 13\u001b[0m, in \u001b[0;36mscrape_sneakers\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.flipkart.com\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Find the search bar and enter \"sneakers\"\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m search_bar \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element_by_name(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m search_bar\u001b[38;5;241m.\u001b[39msend_keys(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msneakers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m search_bar\u001b[38;5;241m.\u001b[39msend_keys(Keys\u001b[38;5;241m.\u001b[39mRETURN)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_element_by_name'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape sneaker data from Flipkart\n",
    "def scrape_sneakers():\n",
    "    # Start the webdriver and open Flipkart\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://www.flipkart.com\")\n",
    "    \n",
    "    # Find the search bar and enter \"sneakers\"\n",
    "    search_bar = driver.find_element_by_name(\"q\")\n",
    "    search_bar.send_keys(\"sneakers\")\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "    \n",
    "    # Scroll down to load more results\n",
    "    for _ in range(5):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Find and scrape the details of the first 100 sneakers\n",
    "    sneakers = []\n",
    "    results = driver.find_elements_by_class_name(\"_1AtVbE\")\n",
    "    for result in results[:100]:\n",
    "        brand = result.find_element_by_class_name(\"_2WkVRV\").text\n",
    "        description = result.find_element_by_class_name(\"IRpwTa\").text\n",
    "        price = result.find_element_by_class_name(\"_30jeq3\").text\n",
    "        sneakers.append({'Brand': brand, 'Description': description, 'Price': price})\n",
    "    \n",
    "    # Close the webdriver\n",
    "    driver.quit()\n",
    "    \n",
    "    return sneakers\n",
    "\n",
    "# Main function to scrape and save sneaker data\n",
    "def main():\n",
    "    sneakers= scrape_sneakers()\n",
    "    df = pd.DataFrame(sneakers)\n",
    "    df.to_csv('sneakers_data.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57afcb4a",
   "metadata": {},
   "source": [
    "Q5: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then set CPU Type filter to “Intel Core i7” as shown in the below image: \n",
    " \n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop: \n",
    "1.\tTitle \n",
    "2.\tRatings \n",
    "3.\tPrice \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2921942",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_element_by_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlaptops_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 51\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[51], line 47\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m---> 47\u001b[0m     laptops \u001b[38;5;241m=\u001b[39m scrape_laptops()\n\u001b[0;32m     48\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(laptops)\n\u001b[0;32m     49\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlaptops_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[51], line 16\u001b[0m, in \u001b[0;36mscrape_laptops\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.amazon.in/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Find the search bar and enter \"Laptop\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m search_bar \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element_by_id(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwotabsearchtextbox\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m search_bar\u001b[38;5;241m.\u001b[39msend_keys(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaptop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m search_bar\u001b[38;5;241m.\u001b[39msend_keys(Keys\u001b[38;5;241m.\u001b[39mRETURN)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_element_by_id'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape laptop data from Amazon.in\n",
    "def scrape_laptops():\n",
    "    # Start the webdriver and open Amazon.in\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://www.amazon.in/\")\n",
    "    \n",
    "    # Find the search bar and enter \"Laptop\"\n",
    "    search_bar = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "    search_bar.send_keys(\"Laptop\")\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "    \n",
    "    # Wait for the CPU Type filter to appear and click on \"Intel Core i7\"\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.LINK_TEXT, \"Intel Core i7\")))\n",
    "    cpu_type_filter = driver.find_element_by_link_text(\"Intel Core i7\")\n",
    "    cpu_type_filter.click()\n",
    "    \n",
    "    # Wait for the results to load\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Find and scrape the details of the first 10 laptops\n",
    "    laptops = []\n",
    "    results = driver.find_elements_by_css_selector(\".s-result-item\")\n",
    "    for result in results[:10]:\n",
    "        try:\n",
    "            title = result.find_element_by_css_selector(\".a-text-normal\").text\n",
    "            ratings = result.find_element_by_css_selector(\".a-icon-star-small\").text\n",
    "            price = result.find_element_by_css_selector(\".a-price-whole\").text\n",
    "            laptops.append({'Title': title, 'Ratings': ratings, 'Price': price})\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Close the webdriver\n",
    "    driver.quit()\n",
    "    \n",
    "    return laptops\n",
    "\n",
    "# Main function to scrape and save laptop data\n",
    "def main():\n",
    "    laptops = scrape_laptops()\n",
    "    df = pd.DataFrame(laptops)\n",
    "    df.to_csv('laptops_data.csv', index=False)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef08fcfa",
   "metadata": {},
   "source": [
    "Q6: Write a python program to scrape data for Top 1000 Quotes of All Time. \n",
    "The above task will be done in following steps: \n",
    "1.\tFirst get the webpagehttps://www.azquotes.com/  \n",
    "2.\tClick on Top Quote \n",
    "3.\tThan scrap a) Quote b) Author c) Type Of Quotes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bad8b996",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"link text\",\"selector\":\"Top Quotes\"}\n  (Session info: chrome=124.0.6367.201); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x00007FF6687A1522+60802]\n\t(No symbol) [0x00007FF66871AC22]\n\t(No symbol) [0x00007FF6685D7CE4]\n\t(No symbol) [0x00007FF668626D4D]\n\t(No symbol) [0x00007FF668626E1C]\n\t(No symbol) [0x00007FF66866CE37]\n\t(No symbol) [0x00007FF66864ABBF]\n\t(No symbol) [0x00007FF66866A224]\n\t(No symbol) [0x00007FF66864A923]\n\t(No symbol) [0x00007FF668618FEC]\n\t(No symbol) [0x00007FF668619C21]\n\tGetHandleVerifier [0x00007FF668AA41BD+3217949]\n\tGetHandleVerifier [0x00007FF668AE6157+3488183]\n\tGetHandleVerifier [0x00007FF668ADF0DF+3459391]\n\tGetHandleVerifier [0x00007FF66885B8E6+823622]\n\t(No symbol) [0x00007FF668725FBF]\n\t(No symbol) [0x00007FF668720EE4]\n\t(No symbol) [0x00007FF668721072]\n\t(No symbol) [0x00007FF6687118C4]\n\tBaseThreadInitThunk [0x00007FFEC4BF257D+29]\n\tRtlUserThreadStart [0x00007FFEC536AA48+40]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome()\n\u001b[0;32m      5\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.azquotes.com/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m top_quotes_button \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mLINK_TEXT,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop Quotes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m top_quotes_button\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m     10\u001b[0m quotes \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.title a\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:741\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    738\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    739\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mFIND_ELEMENT, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m\"\u001b[39m: by, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: value})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"link text\",\"selector\":\"Top Quotes\"}\n  (Session info: chrome=124.0.6367.201); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x00007FF6687A1522+60802]\n\t(No symbol) [0x00007FF66871AC22]\n\t(No symbol) [0x00007FF6685D7CE4]\n\t(No symbol) [0x00007FF668626D4D]\n\t(No symbol) [0x00007FF668626E1C]\n\t(No symbol) [0x00007FF66866CE37]\n\t(No symbol) [0x00007FF66864ABBF]\n\t(No symbol) [0x00007FF66866A224]\n\t(No symbol) [0x00007FF66864A923]\n\t(No symbol) [0x00007FF668618FEC]\n\t(No symbol) [0x00007FF668619C21]\n\tGetHandleVerifier [0x00007FF668AA41BD+3217949]\n\tGetHandleVerifier [0x00007FF668AE6157+3488183]\n\tGetHandleVerifier [0x00007FF668ADF0DF+3459391]\n\tGetHandleVerifier [0x00007FF66885B8E6+823622]\n\t(No symbol) [0x00007FF668725FBF]\n\t(No symbol) [0x00007FF668720EE4]\n\t(No symbol) [0x00007FF668721072]\n\t(No symbol) [0x00007FF6687118C4]\n\tBaseThreadInitThunk [0x00007FFEC4BF257D+29]\n\tRtlUserThreadStart [0x00007FFEC536AA48+40]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "\n",
    "top_quotes_button = driver.find_element(By.LINK_TEXT,\"Top Quotes\")\n",
    "top_quotes_button.click()\n",
    "\n",
    "quotes = driver.find_elements(By.CSS_SELECTOR,\".title a\")\n",
    "authors = driver.find_elements(By.CSS_SELECTOR,\".author a\")\n",
    "types = driver.find_elements(By.CSS_SELECTOR,\".kw-box a\")\n",
    "\n",
    "for quote, author, quote_type in zip(quotes, authors, types):\n",
    "    print(\"Quote:\", quote.text)\n",
    "    print(\"Author:\", author.text)\n",
    "    print(\"Type of Quote:\", quote_type.text)\n",
    "    print()\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c0f17f",
   "metadata": {},
   "source": [
    "Q7: Write a python program to display list of respected former Prime Ministers of India (i.e. Name, Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/general-knowledge/list-ofall-prime-ministers-of-india-1473165149-1  \n",
    " \n",
    " scrap the mentioned data and make the DataFrame \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d889a17c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_element_by_link_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome()\n\u001b[0;32m      6\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.jagranjosh.com/general-knowledge/list-ofall-prime-ministers-of-india-1473165149-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m gk_option \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element_by_link_text(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGK\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m gk_option\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m     11\u001b[0m pm_option \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element_by_link_text(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mList of all Prime Ministers of India\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_element_by_link_text'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://www.jagranjosh.com/general-knowledge/list-ofall-prime-ministers-of-india-1473165149-1\")\n",
    "\n",
    "gk_option = driver.find_element_by_link_text('GK')\n",
    "gk_option.click()\n",
    "\n",
    "pm_option = driver.find_element_by_link_text('List of all Prime Ministers of India')\n",
    "pm_option.click()\n",
    "\n",
    "data = []\n",
    "table = driver.find_element_by_xpath('//table[@class=\"table4\"]')\n",
    "rows = table.find_elements_by_tag_name('tr')\n",
    "for row in rows:\n",
    "  cols = row.find_elements_by_tag_name('td')\n",
    "\n",
    "if len(cols)== 4:\n",
    "    name= cols[0].text\n",
    "    born_dead = cols[1].text\n",
    "    term_of_office = cols[2].text\n",
    "    remarks = cols[3].text\n",
    "    data.append([name, born_dead, term_of_office, remarks])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Name', 'Born-Dead', 'Term of Office', 'Remarks'])\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867a36c1",
   "metadata": {},
   "source": [
    "Q8: Write a python program to display list of 50 Most expensive cars in the world (i.e. Car name and Price) from https://www.motor1.com/  \n",
    " \n",
    "This task will be done in following steps: \n",
    "1.\tFirst get the webpage https://www.motor1.com/ \n",
    "2.\tThen You have to type in the search bar ’50 most expensive cars’ \n",
    "3.\tThen click on 50 most expensive cars in the world.. \n",
    "4.\tThen scrap the mentioned data and make the dataframe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f270f59c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_element_by_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.motor1.com/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Step 2: Type in the search bar\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m search_bar \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element_by_id(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch-input\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m search_bar\u001b[38;5;241m.\u001b[39msend_keys(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m50 most expensive cars\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m search_bar\u001b[38;5;241m.\u001b[39msubmit()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_element_by_id'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "driver = webdriver.Chrome()  \n",
    "\n",
    "# Replace 'path_to_chromedriver' with the actual path to your ChromeDriver executable\n",
    "driver.get('https://www.motor1.com/')\n",
    "\n",
    "# Step 2: Type in the search bar\n",
    "search_bar = driver.find_element_by_id('search-input')\n",
    "search_bar.send_keys('50 most expensive cars')\n",
    "search_bar.submit()\n",
    "\n",
    "# Step 3: Click on the link\n",
    "link = driver.find_element_by_link_text('50 Most Expensive Cars in the World')\n",
    "link.click()\n",
    "\n",
    "# Step 4: Scrape the data and create a dataframe\n",
    "car_names = driver.find_elements_by_xpath('//div[@class=\"article-content\"]/h3')\n",
    "car_prices = driver.find_elements_by_xpath('//div[@class=\"article-content\"]/p')\n",
    "\n",
    "data = []\n",
    "for name, price in zip(car_names, car_prices):\n",
    "    data.append([name.text, price.text])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Car Name', 'Price'])\n",
    "print(df)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e29e4db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
