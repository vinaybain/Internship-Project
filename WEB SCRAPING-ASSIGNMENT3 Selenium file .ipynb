{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503128c2",
   "metadata": {},
   "source": [
    "Write a python program which searches all the product under a particular product from www.amazon.in. The \n",
    "product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for \n",
    "guitars. \n",
    "\n",
    "2. In the above question, now scrape the following details of each product listed in first 3 pages of your search \n",
    "results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then \n",
    "scrape all the products available under that product name. Details to be scraped are: \"Brand \n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and \n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b2aa381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\h.p\\anaconda3\\lib\\site-packages (4.20.0)\n",
      "Collecting webdriver_manager\n",
      "  Obtaining dependency information for webdriver_manager from https://files.pythonhosted.org/packages/b1/51/b5c11cf739ac4eecde611794a0ec9df420d0239d51e73bc19eb44f02b48b/webdriver_manager-4.0.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\h.p\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Collecting python-dotenv (from webdriver_manager)\n",
      "  Obtaining dependency information for python-dotenv from https://files.pythonhosted.org/packages/6a/3e/b68c118422ec867fa7ab88444e1274aa40681c606d59ac27de5a5588f082/python_dotenv-1.0.1-py3-none-any.whl.metadata\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from webdriver_manager) (23.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\h.p\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv, webdriver_manager\n",
      "Successfully installed python-dotenv-1.0.1 webdriver_manager-4.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium webdriver_manager requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b54194d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product to search on Amazon: Guitar\n",
      "No more pages found.\n",
      "Data saved to Guitar_amazon_products.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_product_details(driver):\n",
    "    products = []\n",
    "    product_elements = driver.find_elements(By.XPATH, \"//div[@data-component-type='s-search-result']\")\n",
    "\n",
    "    for product_element in product_elements:\n",
    "        try:\n",
    "            brand_name = product_element.find_element(By.XPATH, \".//span[contains(@class, 'a-size-base-plus')]\").text\n",
    "        except:\n",
    "            brand_name = \"-\"\n",
    "        try:\n",
    "            product_name = product_element.find_element(By.XPATH, \".//h2/a/span\").text\n",
    "        except:\n",
    "            product_name = \"-\"\n",
    "        try:\n",
    "            price = product_element.find_element(By.XPATH, \".//span[@class='a-price-whole']\").text\n",
    "        except:\n",
    "            price = \"-\"\n",
    "        try:\n",
    "            product_url = product_element.find_element(By.XPATH, \".//h2/a\").get_attribute('href')\n",
    "        except:\n",
    "            product_url = \"-\"\n",
    "\n",
    "        # Return/Exchange, Expected Delivery, and Availability usually require going into the product page\n",
    "        # Here, we will provide a placeholder and leave this part for future extension\n",
    "        return_exchange = \"-\"\n",
    "        expected_delivery = \"-\"\n",
    "        availability = \"-\"\n",
    "        \n",
    "        products.append({\n",
    "            \"Brand Name\": brand_name,\n",
    "            \"Name of the Product\": product_name,\n",
    "            \"Price\": price,\n",
    "            \"Return/Exchange\": return_exchange,\n",
    "            \"Expected Delivery\": expected_delivery,\n",
    "            \"Availability\": availability,\n",
    "            \"Product URL\": product_url\n",
    "        })\n",
    "\n",
    "    return products\n",
    "\n",
    "def search_amazon(product):\n",
    "    # Set up the Selenium WebDriver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    driver.get(\"https://www.amazon.in\")\n",
    "    driver.maximize_window()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Find the search box and enter the product name\n",
    "    search_box = driver.find_element(By.ID, \"twotabsearchtextbox\")\n",
    "    search_box.send_keys(product)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    time.sleep(2)\n",
    "\n",
    "    all_products = []\n",
    "    \n",
    "    # Scrape data from the first 3 pages\n",
    "    for page in range(1, 4):\n",
    "        all_products.extend(get_product_details(driver))\n",
    "        \n",
    "        # Go to the next page\n",
    "        try:\n",
    "            next_page = driver.find_element(By.XPATH, \"//a[contains(@class, 's-pagination-next')]\")\n",
    "            next_page.click()\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"No more pages found.\")\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_products)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(f'{product}_amazon_products.csv', index=False)\n",
    "    print(f\"Data saved to {product}_amazon_products.csv\")\n",
    "\n",
    "# Get user input\n",
    "product_to_search = input(\"Enter the product to search on Amazon: \")\n",
    "search_amazon(product_to_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c3968c",
   "metadata": {},
   "source": [
    "Write a python program to access the search bar and search button on images.google.com and scrape 10 \n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae0ab0ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=125.0.6422.113)\nStacktrace:\n\tGetHandleVerifier [0x003DB8E3+45827]\n\t(No symbol) [0x0036DCC4]\n\t(No symbol) [0x0026150F]\n\t(No symbol) [0x0023E133]\n\t(No symbol) [0x002C949F]\n\t(No symbol) [0x002DB8E6]\n\t(No symbol) [0x002C2B96]\n\t(No symbol) [0x00296998]\n\t(No symbol) [0x0029751D]\n\tGetHandleVerifier [0x00694513+2899763]\n\tGetHandleVerifier [0x006E793D+3240797]\n\tGetHandleVerifier [0x004613B4+593364]\n\tGetHandleVerifier [0x004682DC+621820]\n\t(No symbol) [0x003770A4]\n\t(No symbol) [0x003737A8]\n\t(No symbol) [0x00373947]\n\t(No symbol) [0x003659FE]\n\tBaseThreadInitThunk [0x75A77BA9+25]\n\tRtlInitializeExceptionChain [0x77BBBE3B+107]\n\tRtlClearBits [0x77BBBDBF+191]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Search and download images for each keyword\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m keywords:\n\u001b[1;32m---> 68\u001b[0m     search_and_download_images(driver, keyword, num_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Close the driver\u001b[39;00m\n\u001b[0;32m     71\u001b[0m driver\u001b[38;5;241m.\u001b[39mquit()\n",
      "Cell \u001b[1;32mIn[14], line 30\u001b[0m, in \u001b[0;36msearch_and_download_images\u001b[1;34m(driver, keyword, num_images)\u001b[0m\n\u001b[0;32m     28\u001b[0m image_urls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(image_urls) \u001b[38;5;241m<\u001b[39m num_images:\n\u001b[1;32m---> 30\u001b[0m     thumbnails \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg.rg_i\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m thumbnail \u001b[38;5;129;01min\u001b[39;00m thumbnails:\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:771\u001b[0m, in \u001b[0;36mWebDriver.find_elements\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;66;03m# Return empty list if driver returns null\u001b[39;00m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;66;03m# See https://github.com/SeleniumHQ/selenium/issues/4555\u001b[39;00m\n\u001b[1;32m--> 771\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mFIND_ELEMENTS, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m\"\u001b[39m: by, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: value})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=125.0.6422.113)\nStacktrace:\n\tGetHandleVerifier [0x003DB8E3+45827]\n\t(No symbol) [0x0036DCC4]\n\t(No symbol) [0x0026150F]\n\t(No symbol) [0x0023E133]\n\t(No symbol) [0x002C949F]\n\t(No symbol) [0x002DB8E6]\n\t(No symbol) [0x002C2B96]\n\t(No symbol) [0x00296998]\n\t(No symbol) [0x0029751D]\n\tGetHandleVerifier [0x00694513+2899763]\n\tGetHandleVerifier [0x006E793D+3240797]\n\tGetHandleVerifier [0x004613B4+593364]\n\tGetHandleVerifier [0x004682DC+621820]\n\t(No symbol) [0x003770A4]\n\t(No symbol) [0x003737A8]\n\t(No symbol) [0x00373947]\n\t(No symbol) [0x003659FE]\n\tBaseThreadInitThunk [0x75A77BA9+25]\n\tRtlInitializeExceptionChain [0x77BBBE3B+107]\n\tRtlClearBits [0x77BBBDBF+191]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Function to search and download images\n",
    "def search_and_download_images(driver, keyword, num_images=10):\n",
    "    # Create a directory for the keyword if it doesn't exist\n",
    "    if not os.path.exists(keyword):\n",
    "        os.makedirs(keyword)\n",
    "    \n",
    "    # Access Google Images\n",
    "    driver.get(\"https://images.google.com\")\n",
    "    \n",
    "    # Find the search bar, input the keyword, and trigger the search\n",
    "    search_box = driver.find_element(By.NAME, \"q\")\n",
    "    search_box.send_keys(keyword)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    \n",
    "    # Wait for the images to load\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Scrape image URLs\n",
    "    image_urls = set()\n",
    "    while len(image_urls) < num_images:\n",
    "        thumbnails = driver.find_elements(By.CSS_SELECTOR, \"img.rg_i\")\n",
    "        for thumbnail in thumbnails:\n",
    "            try:\n",
    "                thumbnail.click()\n",
    "                time.sleep(1)\n",
    "                images = driver.find_elements(By.CSS_SELECTOR, \"img.n3VNCb\")\n",
    "                for image in images:\n",
    "                    if image.get_attribute(\"src\") and 'http' in image.get_attribute(\"src\"):\n",
    "                        image_urls.add(image.get_attribute(\"src\"))\n",
    "                    if len(image_urls) >= num_images:\n",
    "                        break\n",
    "                if len(image_urls) >= num_images:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Download images\n",
    "    for i, url in enumerate(image_urls):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            with open(os.path.join(keyword, f\"{keyword}_{i+1}.jpg\"), \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not download {url} - {e}\")\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up the Selenium WebDriver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    \n",
    "    # List of keywords to search for\n",
    "    keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "    \n",
    "    # Search and download images for each keyword\n",
    "    for keyword in keywords:\n",
    "        search_and_download_images(driver, keyword, num_images=10)\n",
    "    \n",
    "    # Close the driver\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760e53b9",
   "metadata": {},
   "source": [
    "Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
    "and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand \n",
    "Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, \n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the \n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "074b13c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the smartphone to search: Motorola\n",
      "Data scraped and saved to smartphones.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Open Flipkart\n",
    "driver.get(\"https://www.flipkart.com\")\n",
    "time.sleep(2)  # wait for the page to load\n",
    "\n",
    "# Close the login popup if it appears\n",
    "try:\n",
    "    close_login_popup = driver.find_element(By.XPATH, \"//button[contains(text(),'✕')]\")\n",
    "    close_login_popup.click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Take user input for the smartphone to search\n",
    "search_query = input(\"Enter the smartphone to search: \")\n",
    "\n",
    "# Search for the smartphone\n",
    "search_bar = driver.find_element(By.NAME, \"q\")\n",
    "search_bar.clear()\n",
    "search_bar.send_keys(search_query)\n",
    "search_bar.send_keys(Keys.RETURN)\n",
    "time.sleep(3)  # wait for the search results to load\n",
    "\n",
    "# Scrape the data\n",
    "smartphones = []\n",
    "products = driver.find_elements(By.XPATH, \"//div[@class='_1AtVbE']\")\n",
    "\n",
    "for product in products:\n",
    "    try:\n",
    "        brand_name = product.find_element(By.XPATH, \".//div[@class='_4rR01T']\").text.split()[0]\n",
    "    except:\n",
    "        brand_name = \"-\"\n",
    "    try:\n",
    "        smartphone_name = product.find_element(By.XPATH, \".//div[@class='_4rR01T']\").text\n",
    "    except:\n",
    "        smartphone_name = \"-\"\n",
    "    try:\n",
    "        color = product.find_element(By.XPATH, \".//div[@class='fMghEO']//li[1]\").text.split(':')[1].strip()\n",
    "    except:\n",
    "        color = \"-\"\n",
    "    try:\n",
    "        ram = product.find_element(By.XPATH, \".//div[@class='fMghEO']//li[2]\").text.split('|')[0].strip().split()[0]\n",
    "    except:\n",
    "        ram = \"-\"\n",
    "    try:\n",
    "        storage = product.find_element(By.XPATH, \".//div[@class='fMghEO']//li[2]\").text.split('|')[1].strip()\n",
    "    except:\n",
    "        storage = \"-\"\n",
    "    try:\n",
    "        primary_camera = product.find_element(By.XPATH, \".//div[@class='fMghEO']//li[3]\").text.split('|')[0].strip()\n",
    "    except:\n",
    "        primary_camera = \"-\"\n",
    "    try:\n",
    "        secondary_camera = product.find_element(By.XPATH, \".//div[@class='fMghEO']//li[3]\").text.split('|')[1].strip()\n",
    "    except:\n",
    "        secondary_camera = \"-\"\n",
    "    try:\n",
    "        display_size = product.find_element(By.XPATH, \".//div[@class='fMghEO']//li[4]\").text.strip()\n",
    "    except:\n",
    "        display_size = \"-\"\n",
    "    try:\n",
    "        battery_capacity = product.find_element(By.XPATH, \".//div[@class='fMghEO']//li[5]\").text.strip()\n",
    "    except:\n",
    "        battery_capacity = \"-\"\n",
    "    try:\n",
    "        price = product.find_element(By.XPATH, \".//div[@class='_30jeq3 _1_WHN1']\").text\n",
    "    except:\n",
    "        price = \"-\"\n",
    "    try:\n",
    "        product_url = product.find_element(By.XPATH, \".//a\").get_attribute(\"href\")\n",
    "    except:\n",
    "        product_url = \"-\"\n",
    "\n",
    "    smartphones.append({\n",
    "        \"Brand Name\": brand_name,\n",
    "        \"Smartphone Name\": smartphone_name,\n",
    "        \"Colour\": color,\n",
    "        \"RAM\": ram,\n",
    "        \"Storage(ROM)\": storage,\n",
    "        \"Primary Camera\": primary_camera,\n",
    "        \"Secondary Camera\": secondary_camera,\n",
    "        \"Display Size\": display_size,\n",
    "        \"Battery Capacity\": battery_capacity,\n",
    "        \"Price\": price,\n",
    "        \"Product URL\": product_url\n",
    "    })\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame and save to CSV\n",
    "df = pd.DataFrame(smartphones)\n",
    "df.to_csv(\"smartphones.csv\", index=False)\n",
    "\n",
    "print(\"Data scraped and saved to smartphones.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f975d189",
   "metadata": {},
   "source": [
    "Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d65f79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the city to search for: Sagar\n",
      "Latitude: 23.8374638, Longitude: 78.6662223\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Initialize the WebDriver (assuming you have ChromeDriver installed)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open Google Maps\n",
    "driver.get(\"https://www.google.com/maps\")\n",
    "\n",
    "# Search for the city\n",
    "city = input(\"Enter the city to search for: \")\n",
    "search_box = driver.find_element(By.ID, \"searchboxinput\")\n",
    "search_box.send_keys(city)\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for the map to update\n",
    "time.sleep(5)\n",
    "\n",
    "# Get the current URL\n",
    "current_url = driver.current_url\n",
    "\n",
    "# Extract the latitude and longitude from the URL\n",
    "try:\n",
    "    coords = current_url.split(\"@\")[1].split(\",\")\n",
    "    latitude = coords[0]\n",
    "    longitude = coords[1]\n",
    "    print(f\"Latitude: {latitude}, Longitude: {longitude}\")\n",
    "except IndexError:\n",
    "    print(\"Failed to extract coordinates from the URL.\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52744c0d",
   "metadata": {},
   "source": [
    "Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25fa1e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been scraped and saved to best_gaming_laptops.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "\n",
    "# Set up the WebDriver\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Run in headless mode (without opening a browser window)\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "\n",
    "# Open the webpage\n",
    "driver.get(url)\n",
    "\n",
    "# Find the container with laptop details\n",
    "laptop_containers = driver.find_elements(By.CSS_SELECTOR, '.TopNumbeHeading')\n",
    "\n",
    "# Initialize lists to store the scraped data\n",
    "titles = []\n",
    "specs = []\n",
    "descriptions = []\n",
    "\n",
    "# Iterate through each laptop container and extract details\n",
    "for laptop in laptop_containers:\n",
    "    # Extract the title\n",
    "    title = laptop.find_element(By.TAG_NAME, 'h2').text.strip()\n",
    "    titles.append(title)\n",
    "\n",
    "    # Extract the specifications and description\n",
    "    spec_container = laptop.find_element(By.XPATH, 'following-sibling::div[@class=\"Spcs-details\"]')\n",
    "    spec_list = spec_container.find_element(By.CLASS_NAME, 'SpecsBlk').text.strip()\n",
    "    description = spec_container.find_element(By.CLASS_NAME, 'Spcs-Descrp').text.strip()\n",
    "\n",
    "    specs.append(spec_list)\n",
    "    descriptions.append(description)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame to store the scraped data\n",
    "data = {\n",
    "    'Title': titles,\n",
    "    'Specifications': specs,\n",
    "    'Description': descriptions\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv('best_gaming_laptops.csv', index=False)\n",
    "\n",
    "print(\"Data has been scraped and saved to best_gaming_laptops.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633cb751",
   "metadata": {},
   "source": [
    "Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: \n",
    "“Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7635fc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped and saved to forbes_billionaires.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# Define the URL of the Forbes billionaires list\n",
    "url = \"https://www.forbes.com/billionaires/\"\n",
    "\n",
    "# Load the page\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the dynamic content to load (you may need to adjust the wait time)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Find the table rows containing billionaire data\n",
    "rows = driver.find_elements(By.XPATH, \"//div[@class='fullList']//div[@class='card']\")\n",
    "\n",
    "# Initialize an empty list to store billionaire data\n",
    "billionaires_data = []\n",
    "\n",
    "# Iterate over each row and extract the data\n",
    "for row in rows:\n",
    "    rank = row.find_element(By.XPATH, \".//div[@class='rank']\").text\n",
    "    name = row.find_element(By.XPATH, \".//div[@class='personName']\").text\n",
    "    net_worth = row.find_element(By.XPATH, \".//div[@class='netWorth']\").text\n",
    "    age = row.find_element(By.XPATH, \".//div[@class='age']\").text\n",
    "    citizenship = row.find_element(By.XPATH, \".//div[@class='countryOfCitizenship']\").text\n",
    "    source = row.find_element(By.XPATH, \".//div[@class='source']\").text\n",
    "    industry = row.find_element(By.XPATH, \".//div[@class='industry']\").text\n",
    "\n",
    "    # Append the data to the list\n",
    "    billionaires_data.append({\n",
    "        \"Rank\": rank,\n",
    "        \"Name\": name,\n",
    "        \"Net worth\": net_worth,\n",
    "        \"Age\": age,\n",
    "        \"Citizenship\": citizenship,\n",
    "        \"Source\": source,\n",
    "        \"Industry\": industry\n",
    "    })\n",
    "\n",
    "# Convert the list of billionaire data to a pandas DataFrame\n",
    "df = pd.DataFrame(billionaires_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('forbes_billionaires.csv', index=False)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "print(\"Data scraped and saved to forbes_billionaires.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba499a",
   "metadata": {},
   "source": [
    "Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted \n",
    "from any YouTube Video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc78c920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# Define the URL of the YouTube video\n",
    "video_url = \"https://www.youtube.com/\"\n",
    "\n",
    "# Load the page\n",
    "driver.get(video_url)\n",
    "\n",
    "# Wait for the comments to load (you may need to adjust the wait time)\n",
    "time.sleep(10)\n",
    "\n",
    "# Scroll down to load more comments (you may need to adjust the number of scrolls)\n",
    "scroll_pause_time = 2  # Waiting time after each scroll\n",
    "last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "while True:\n",
    "    # Scroll down to the bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    # Wait for the comments to load\n",
    "    time.sleep(scroll_pause_time)\n",
    "    # Calculate new scroll height and compare with the last one\n",
    "    new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Find all comments\n",
    "comments = driver.find_elements(By.XPATH, \"//ytd-comment-thread-renderer\")\n",
    "\n",
    "# Initialize an empty list to store comments data\n",
    "comments_data = []\n",
    "\n",
    "# Iterate over each comment and extract the data\n",
    "for comment in comments:\n",
    "    try:\n",
    "        comment_text = comment.find_element(By.XPATH, \".//yt-formatted-string[@id='content-text']\").text\n",
    "        upvotes = comment.find_element(By.XPATH, \".//span[@id='vote-count-middle']\").text\n",
    "        timestamp = comment.find_element(By.XPATH, \".//span[@class='style-scope ytd-comment-renderer']//yt-formatted-string\").text\n",
    "\n",
    "        # Append the data to the list\n",
    "        comments_data.append({\n",
    "            \"Comment\": comment_text,\n",
    "            \"Upvotes\": upvotes,\n",
    "            \"Timestamp\": timestamp\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing comment: {e}\")\n",
    "\n",
    "# Print the first 10 comments data (for demonstration purposes)\n",
    "for comment in comments_data[:10]:\n",
    "    print(comment)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "print(\"Data extraction complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d8f26c",
   "metadata": {},
   "source": [
    "Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in \n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall \n",
    "reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62ebcb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraping complete. Data saved to london_hostels.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# Define the URL of the Hostelworld search results for London\n",
    "url = \"https://www.hostelworld.com/search?q=London%2C+England&country=England&city=London&type=city&from=2023-04-01&to=2023-04-02&guests=1&pricesfrom=0&pricesto=1000&currency=USD&language=en&page=\"\n",
    "\n",
    "# Initialize an empty list to store hostel data\n",
    "hostels_data = []\n",
    "\n",
    "# Function to scrape data from each hostel page\n",
    "def scrape_hostel_data(driver, url):\n",
    "    driver.get(url)\n",
    "    hostels = driver.find_elements(By.XPATH, \"//div[@class='property-card-wrapper']\")\n",
    "    for hostel in hostels:\n",
    "        try:\n",
    "            name = hostel.find_element(By.XPATH, \".//div[@class='property-card-title']\").text\n",
    "            distance = hostel.find_element(By.XPATH, \".//div[@class='distance']\").text\n",
    "            rating = hostel.find_element(By.XPATH, \".//div[@class='rating']\").text\n",
    "            total_reviews = hostel.find_element(By.XPATH, \".//div[@class='reviews']\").text\n",
    "            # ... other data points ...\n",
    "            # Add more XPaths and code to extract the rest of the required data\n",
    "            # ...\n",
    "            hostels_data.append({\n",
    "                \"Name\": name,\n",
    "                \"Distance\": distance,\n",
    "                \"Rating\": rating,\n",
    "                \"Total Reviews\": total_reviews,\n",
    "                # ... other data points ...\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing hostel: {e}\")\n",
    "\n",
    "# Scrape data from multiple pages\n",
    "for page in range(1, 11):  # Adjust the range based on the number of pages you want to scrape\n",
    "    scrape_hostel_data(driver, url + str(page))\n",
    "\n",
    "# Convert the list of hostel data to a pandas DataFrame\n",
    "df = pd.DataFrame(hostels_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('london_hostels.csv', index=False)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "print(\"Data scraping complete. Data saved to london_hostels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff333c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
